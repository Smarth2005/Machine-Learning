{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCGGuu89abjDWNSjmnfRh4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Smarth2005/Machine-Learning/blob/main/Data%20Preprocessing/Feature%20Scaling%20and%20Transformation/1.%20Feature%20Scaling%20Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Equal Footing: The Role of Feature Scaling in ML Models**\n",
        "\n",
        "<div align=\"justify\">\n",
        "\n",
        "In real-world datasets, different features often have different units, ranges, and magnitudes. For example, a feature like `age` may range from 0 to 100, while `income` could be in the thousands. Such imbalance can cause machine learning modelsâ€”especially those based on **distance** or **gradient optimization**â€”to become **biased toward features with larger numeric values**.\n",
        "\n",
        "> Feature scaling solves this by bringing all features onto a **comparable scale**, ensuring each one contributes fairly to the modelâ€™s learning process. This is known as putting features on \"equal footing\".\n",
        "\n",
        "### ðŸ“Œ <u>When Is Feature Scaling Needed?</u>\n",
        "\n",
        "âœ… Required for models that rely on distance calculations, dot products, or gradients:\n",
        "- Linear Regression / Logistic Regression\n",
        "- Support Vector Machines (SVM)\n",
        "- Principal Component Analysis (PCA)\n",
        "- Linear Discriminant Analysis (LDA)\n",
        "- K-Nearest Neighbors (KNN)\n",
        "- K-Means Clustering\n",
        "- Neural Networks (faster training)\n",
        "\n",
        "âŒ Not Needed for Tree-based models, which use threshold-based splits (e.g., `feature > value`)\n",
        "- Decision Trees\n",
        "- Random Forests\n",
        "- Gradient Boosting Trees (XGBoost, LightGBM, etc.)\n",
        "\n",
        "### ðŸŽ¯ <u>Why is Feature Scaling Important ?</u>\n",
        "\n",
        "- Prevents dominance of large-scale features.\n",
        "- Speeds up convergence in optimization algorithms like gradient descent.\n",
        "- Improves model performance and stability in many algorithms.\n",
        "- Helps uncover hidden patterns by aligning feature distributions.\n",
        "\n",
        "### <u>Feature Scaling Techniques:</u>\n",
        "\n",
        "- `StandardScaler`\n",
        "- `MinMaxScaler`\n",
        "- `RobustScaler`\n",
        "- `MaxAbsScaler`\n",
        "- `Normalizer`\n",
        "\n",
        "We will explore and implement these feature scaling techniques in the next set of notebooks.\n",
        "\n",
        "<span style=\"color:red;\"><b>Important Note:</b> Feature Scaling is used to normalize the range or scale of features. It doesn't change the shape of the distribution.</span>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "qjsR0j-H3SGy"
      }
    }
  ]
}